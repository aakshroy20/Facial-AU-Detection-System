{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FACS (Tensorflow core).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "REfITB_LYVct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from pandas import DataFrame\n",
        " \n",
        "import pandas as pd\n",
        " \n",
        "x = pd.read_csv(\"predx_for_regression.csv\", header=0)\n",
        "\n",
        "x_array = np.asarray(x, dtype = \"float\")\n",
        "\n",
        "x_arrayt=x_array\n",
        "\n",
        "y = pd.read_csv(\"predy_for_regression.csv\", header=0)\n",
        "y_array = np.asarray(y, dtype = \"float\")\n",
        "y_arrayt=y_array\n",
        "\n",
        "whole_data=np.concatenate((x_arrayt, y_arrayt),axis=1)\n",
        "\n",
        "angle = pd.read_csv(\"angle.csv\", header=0)\n",
        "angle_array = np.asarray(angle, dtype = \"float\")\n",
        "\n",
        "angle_arrayt=angle_array.transpose()\n",
        "\n",
        "#Network parameters\n",
        "n_hidden1 = 147\n",
        "n_hidden2 = 74\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_input = 98\n",
        "n_output = 5\n",
        "#Learning parameters\n",
        "learning_constant = 0.03#0.002\n",
        "number_epochs = 200#5000#10000\n",
        "batch_size = 10000 \n",
        "\n",
        "#Defining the input and the output\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_output])\n",
        "\n",
        "#DEFINING WEIGHTS AND BIASESï¼Œinitailly, they are just random values\n",
        "\n",
        "\n",
        "#Biases first hidden layer\n",
        "b1 = tf.Variable(tf.random_normal([n_hidden1]))\n",
        "#Biases second hidden layer\n",
        "b2 = tf.Variable(tf.random_normal([n_hidden2]))\n",
        "#Biases output layer\n",
        "b3 = tf.Variable(tf.random_normal([n_hidden3]))\n",
        "########################################################\n",
        "b4 = tf.Variable(tf.random_normal([n_hidden4]))\n",
        "########################################################\n",
        "#Biases output layer\n",
        "b5 = tf.Variable(tf.random_normal([n_output]))\n",
        "\n",
        "\n",
        "#Weights connecting input layer with first hidden layer\n",
        "w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))\n",
        "#Weights connecting first hidden layer with second hidden layer\n",
        "w2 = tf.Variable(tf.random_normal([n_hidden1, n_hidden2]))\n",
        "#Weights connecting second hidden layer with output layer\n",
        "w3 = tf.Variable(tf.random_normal([n_hidden2, n_hidden3]))\n",
        "###########################################################\n",
        "w4 = tf.Variable(tf.random_normal([n_hidden3, n_hidden4]))\n",
        "##########################################################\n",
        "w5 = tf.Variable(tf.random_normal([n_hidden4, n_output]))\n",
        "\n",
        "\n",
        "\n",
        "#The incoming data given to the\n",
        "#network is input_d\n",
        "def multilayer_perceptron(input_d):\n",
        "    #drop out###########################################\n",
        "    w1_temp= tf.nn.dropout(w1, 0.5)\n",
        "    w2_temp= tf.nn.dropout(w2, 0.5)\n",
        "    w3_temp= tf.nn.dropout(w3, 0.5)\n",
        "    w4_temp= tf.nn.dropout(w4, 0.5)\n",
        "    ############################################\n",
        "    #Task of neurons of first hidden layer\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(input_d, w1_temp), b1))\n",
        "    #Task of neurons of second hidden layer\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w2_temp), b2))\n",
        "    #Task of neurons of output layer\n",
        "    layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, w3_temp), b3))\n",
        "    #########################################################\n",
        "    #Task of neurons of output layer\n",
        "    layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, w4_temp), b4))\n",
        "    #########################################################\n",
        "    #Task of neurons of output layer\n",
        "    out_layer = tf.add(tf.matmul(layer_4, w5),b5)\n",
        "    \n",
        "    return out_layer\n",
        "\n",
        "#Create model\n",
        "neural_network = multilayer_perceptron(X)\n",
        "\n",
        "#Define loss and optimizer\n",
        "loss_op = tf.sqrt(tf.reduce_mean(tf.math.squared_difference(neural_network,Y)))+ 0.01*tf.nn.l2_loss(w1) + 0.01*tf.nn.l2_loss(w2) + 0.01*tf.nn.l2_loss(w3) + 0.01*tf.nn.l2_loss(w4) \n",
        "# reduce_mean didn't set axis, to get the overall average value. the outcome is a number in this case\n",
        "\n",
        "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_network,labels=Y))\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
        " optimizer = tf.train.AdamOptimizer(learning_constant).minimize(loss_op)\n",
        "\n",
        "#Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "label=angle_array\n",
        "\n",
        "batch_x=(whole_data-200)/2000 #(367616, 98)\n",
        "# temp=np.array([angle_array[:,0]])##*only consider the first column of the angle    (1, 367616)\n",
        "# batch_y=temp.transpose() #( 367616, 1)\n",
        "batch_y=label\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "batch_x_train,batch_x_test,batch_y_train,batch_y_test=train_test_split(batch_x,batch_y,test_size=0.3)\n",
        "\n",
        "label_train,label_test=train_test_split(label,test_size=0.3)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    #Training epoch\n",
        "    for epoch in range(number_epochs):\n",
        "        sess.run(optimizer, feed_dict={X: batch_x_train, Y: batch_y_train})\n",
        "        \n",
        "        #Display the epoch\n",
        "        if epoch % 100 == 0 and epoch>10:\n",
        "            print(\"Epoch:\", '%d' % (epoch))\n",
        "            ac = loss_op.eval({X: batch_x_train, Y: batch_y_train})\n",
        "            print(\"Accuracy:\",  ac)\n",
        "    ##kfole########################\n",
        "    print('doing kfold validation')\n",
        "    Accuracys_list = []\n",
        "    k = 10\n",
        "    kf = KFold(n_splits=k)\n",
        "    for train_index, test_index in kf.split(label):\n",
        "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "        X_train, X_test = batch_x_train[train_index], batch_x_train[test_index]\n",
        "        y_train, y_test = batch_y_train[train_index], batch_y_train[test_index]\n",
        "        for epoch in range(number_epochs):\n",
        "            sess.run(optimizer, feed_dict={X: X_train, Y: y_train})\n",
        "        ac = loss_op.eval({X: X_test, Y: y_test})\n",
        "        Accuracys_list.add(ac)\n",
        "    \n",
        "    print(\"Average Root Mean Square Error: \",Accuracys_list/k)\n",
        "    print('kfold validation finished')\n",
        "    ##############################\n",
        "    \n",
        "    # Test model\n",
        "    pred = (neural_network)  # Apply softmax to logits\n",
        "    accuracy=tf.keras.losses.MSE(pred,Y)#mse:short for \"mean square error\"\n",
        "    print(\"MeanSquareError: Accuracy:\", np.square(accuracy.eval({X: batch_x_train, Y: batch_y_train})).mean() )\n",
        "    #tf.keras.evaluate(pred,batch_x)\n",
        "    \n",
        "    ###############################################\n",
        "    \n",
        "    accuracy=tf.keras.losses.MAE(pred,Y)\n",
        "    print(\"MeanAbsoluteError: Accuracy:\", np.square(accuracy.eval({X: batch_x_train, Y: batch_y_train})).mean() )\n",
        "    accuracy=tf.keras.losses.MAPE(pred,Y)\n",
        "    print(\"MeanAbsolutePercentageError: Accuracy:\", np.square(accuracy.eval({X: batch_x_train, Y: batch_y_train})).mean() )\n",
        "    accuracy=tf.keras.losses.MSLE(pred,Y)\n",
        "    print(\"MeanSquaredLogarithmicError: Accuracy:\", np.square(accuracy.eval({X: batch_x_train, Y: batch_y_train})).mean() )\n",
        "    print(\"Prediction:\", pred.eval({X: batch_x_train}))\n",
        "    print(batch_y)\n",
        "\n",
        "    #############################################\n",
        "    \n",
        "    output=neural_network.eval({X: batch_x_train})\n",
        "    plt.plot(batch_y_train[:,0], 'r', output[:,0], 'b')\n",
        "    plt.ylabel(\"X's prediction vs true value\")\n",
        "    plt.show()\n",
        "    \n",
        "    output=neural_network.eval({X: batch_x_train})\n",
        "    plt.plot(batch_y_train[:,1], 'r', output[:,1], 'b')\n",
        "    plt.ylabel(\"Y's prediction vs true value\")\n",
        "    plt.show()\n",
        "    \n",
        "    output=neural_network.eval({X: batch_x_train})\n",
        "    plt.plot(batch_y_train[:,2], 'r', output[:,2], 'b')\n",
        "    plt.ylabel(\"Z's prediction vs true value\")\n",
        "    plt.show()\n",
        "    \n",
        "    #############################################\n",
        "    \n",
        "   \n",
        "    df = DataFrame(output)\n",
        "\n",
        "    export_csv = df.to_csv ('output.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n",
        "\n",
        "    print (df)\n",
        "    #Create a session"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}